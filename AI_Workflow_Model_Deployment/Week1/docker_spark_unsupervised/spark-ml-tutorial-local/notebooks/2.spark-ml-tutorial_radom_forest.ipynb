{
 "cells": [
  {
   "attachments": {
    "ibm-cloud.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![ibm-cloud.png](attachment:ibm-cloud.png)\n",
    "\n",
    "# Spark  Supervised Machine Learning\n",
    "\n",
    "Keep the main [Spark ML documentation](https://spark.apache.org/docs/latest/ml-pipeline.html) as you go through this tutorial.  MLlib is Sparkâ€™s machine learning (ML) library. **Spark ML** is not an official name, but we will use it to refer to the MLlib DataFrame-based API that embraces ML pipelines. Before we get into Spark ML by demonstrating a couple of examples we will first review Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\",\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and **puts a schema on them**.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as **RDDs with schema**.\n",
    "\n",
    "What are **schemas**?\n",
    "- Schemas are metadata about your data.\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using **column names** instead of column positions\n",
    "- Schemas enable **queries** using SQL and DataFrame syntax\n",
    "- Schemas make your data more **structured**.\n",
    "\n",
    "See the [Spark SQL documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html) as a main point of reference for Spark SQL, DataFrames and Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module.  \n",
    "\n",
    "<center>\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th>Types</th>\n",
    "      <th>Python Equivalent</th>\n",
    "    </tr>\n",
    "  <tr>\n",
    "      <td>StringType</td>\n",
    "      <td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td>IntegerType</td>\n",
    "      <td>integer</td>\n",
    "   <tr>\n",
    "      <td>FloatType</td>\n",
    "      <td>float</td> \n",
    "  <tr>\n",
    "      <td>ArrayType</td>\n",
    "      <td>array or list</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td>MapType</td>\n",
    "      <td>dict</td>\n",
    "   </tr>       \n",
    "</table>\n",
    "</center>\n",
    "\n",
    "First we initialize the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"spark-ml-examples\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `local[4]` will create a `local` cluster made of the driver using all 4 cores.  Lets start with a very small file to to demonstrate the different ways to create Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(111, '10/13/2019', 4, 'US', 1),\n",
       " (122, '10/15/2019', 5, 'SG', 1),\n",
       " (102, '10/16/2019', 11, 'US', 1),\n",
       " (144, '10/25/2019', 14, 'US', 2),\n",
       " (121, '10/26/2019', 7, 'SG', 1),\n",
       " (155, '10/27/2019', 9, 'US', 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(args):\n",
    "    user_id, date, num_streams, country, invoice_item = args\n",
    "    return((int(user_id), date, int(num_streams), country, int(invoice_item)))\n",
    "\n",
    "rdd_aavail = sc.textFile(os.path.join(DATA_DIR, 'example-data.csv'))\\\n",
    "                         .map(lambda rowstr : rowstr.split(\",\"))\\\n",
    "                         .filter(lambda row: not row[0].startswith('#'))\\\n",
    "                         .map(casting_function)\n",
    "\n",
    "rdd_aavail.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can create a Spark DataFrame using a schema that you have defined or it can be inferred.  To create your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-------+-------------+\n",
      "|user_id|      date|num_streams|country|invoice_items|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "|    111|10/13/2019|          4|     US|            1|\n",
      "|    122|10/15/2019|          5|     SG|            1|\n",
      "|    102|10/16/2019|         11|     US|            1|\n",
      "|    144|10/25/2019|         14|     US|            2|\n",
      "|    121|10/26/2019|          7|     SG|            1|\n",
      "|    155|10/27/2019|          9|     US|            3|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_items: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('num_streams', IntegerType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('invoice_items', IntegerType(), True) ])\n",
    "    \n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_aavail, schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also read the data directly from a file and **infer** the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line count: 6\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n",
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv(os.path.join(DATA_DIR, 'example-data.csv'),\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a nice format\n",
    "df.show()\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn the DataFrame into a Panda DataFrame, but be careful since this 'action' will put all the data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user</th>\n",
       "      <th>date</th>\n",
       "      <th>num_streams</th>\n",
       "      <th>country</th>\n",
       "      <th>invoice_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>10/13/2019</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>10/15/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>10/16/2019</td>\n",
       "      <td>11</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144</td>\n",
       "      <td>10/25/2019</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>7</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>10/27/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #user        date  num_streams country  invoice_item\n",
       "0    111  10/13/2019            4      US             1\n",
       "1    122  10/15/2019            5      SG             1\n",
       "2    102  10/16/2019           11      US             1\n",
       "3    144  10/25/2019           14      US             2\n",
       "4    121  10/26/2019            7      SG             1\n",
       "5    155  10/27/2019            9      US             3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common operations that you might perform on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|summary|             #user|      date|      num_streams|country|      invoice_item|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|  count|                 6|         6|                6|      6|                 6|\n",
      "|   mean|125.83333333333333|      null|8.333333333333334|   null|               1.5|\n",
      "| stddev|20.034137532388726|      null|3.777124126457412|   null|0.8366600265340756|\n",
      "|    min|               102|10/13/2019|                4|     SG|                 1|\n",
      "|    max|               155|10/27/2019|               14|     US|                 3|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+-----------------+\n",
      "|summary|      num_streams|\n",
      "+-------+-----------------+\n",
      "|  count|                6|\n",
      "|   mean|8.333333333333334|\n",
      "| stddev|3.777124126457412|\n",
      "|    min|                4|\n",
      "|    max|               14|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df.describe(\"num_streams\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "\n",
    "Lets read in in the AAVAIL dataset that we have been working with to demonstrate the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|summary|      customer_id|     is_subscriber|      country|               age| customer_name| subscriber_type|      num_streams|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|  count|             1000|              1000|         1000|              1000|          1000|            1000|             1000|\n",
      "|   mean|            500.5|             0.711|         null|            25.325|          null|            null|           17.695|\n",
      "| stddev|288.8194360957494|0.4535247343692345|         null|12.184655959067568|          null|            null|4.798020007877829|\n",
      "|    min|                1|                 0|    singapore|               -50|Aaliyah Duarte|    aavail_basic|                1|\n",
      "|    max|             1000|                 1|united_states|                50|   Zoie Cortes|aavail_unlimited|               29|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aavail = spark.read.csv(os.path.join(DATA_DIR, 'aavail-target.csv'),\n",
    "                           header=True,       \n",
    "                           quote='\"',         \n",
    "                           sep=\",\",          \n",
    "                           inferSchema=True)\n",
    "df_aavail.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Remove one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|summary|     is_subscriber|      country|               age| subscriber_type|      num_streams|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|  count|              1000|         1000|              1000|            1000|             1000|\n",
      "|   mean|             0.711|         null|            25.325|            null|           17.695|\n",
      "| stddev|0.4535247343692345|         null|12.184655959067568|            null|4.798020007877829|\n",
      "|    min|                 0|    singapore|               -50|    aavail_basic|                1|\n",
      "|    max|                 1|united_states|                50|aavail_unlimited|               29|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "\n",
      "+----------------+-----+\n",
      "| subscriber_type|count|\n",
      "+----------------+-----+\n",
      "|  aavail_premium|  331|\n",
      "|aavail_unlimited|  302|\n",
      "|    aavail_basic|  367|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['customer_id', 'customer_name']\n",
    "df_aavail = df_aavail.drop(*columns_to_drop)\n",
    "df_aavail.describe().show()\n",
    "df_aavail.groupBy(\"subscriber_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on a feature matrix\n",
    "\n",
    "The following example demonstrates how to deal with categorical features and scale continuous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## scale the continuous features\n",
    "va = VectorAssembler(inputCols=[\"age\", \"num_streams\"], outputCol=\"cont_features\")\n",
    "ss = standardScaler = StandardScaler(inputCol=\"cont_features\", outputCol=\"cont_scaled\")\n",
    "\n",
    "## categorical variable transformation\n",
    "cat_cols = [\"country\", \"subscriber_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_oh\") for column in cat_cols]\n",
    "\n",
    "## assemple the features for input into the ML model\n",
    "assembler = VectorAssembler(inputCols=[\"cont_scaled\", \"country_oh\", \"subscriber_type_oh\"], outputCol=\"features\")\n",
    "\n",
    "## setup a model\n",
    "gbt = RandomForestClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\", numTrees = 50)\n",
    "paramMap = {gbt.maxDepth: 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the pipeline and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[1.72347910934425...|            1|[8.70124118427482...|[0.17402482368549...|       1.0|\n",
      "|(5,[0,1],[2.46211...|            0|[36.7700518799553...|[0.73540103759910...|       0.0|\n",
      "|[1.72347910934425...|            0|[8.61953763918798...|[0.17239075278375...|       1.0|\n",
      "|[1.64140867556596...|            1|[6.89093554046689...|[0.13781871080933...|       1.0|\n",
      "|[1.72347910934425...|            1|[31.3400927147338...|[0.62680185429467...|       0.0|\n",
      "|[1.72347910934425...|            1|[8.02005465412437...|[0.16040109308248...|       1.0|\n",
      "|[3.93938082135830...|            0|[25.3381507231179...|[0.50676301446235...|       0.0|\n",
      "|[3.85731038758001...|            1|[8.40306850172952...|[0.16806137003459...|       1.0|\n",
      "|[1.72347910934425...|            0|[10.1574223610511...|[0.20314844722102...|       1.0|\n",
      "|[2.13383127823575...|            0|[7.27254791638341...|[0.14545095832766...|       1.0|\n",
      "|[1.14898607289617...|            1|[8.83397794524160...|[0.17667955890483...|       1.0|\n",
      "|(5,[0,1],[3.20074...|            0|[40.1475099249654...|[0.80295019849930...|       0.0|\n",
      "|[1.64140867556596...|            1|[8.14648657900700...|[0.16292973158014...|       1.0|\n",
      "|[1.55933824178766...|            1|[7.46647137584567...|[0.14932942751691...|       1.0|\n",
      "|[1.80554954312255...|            0|[8.05293769713895...|[0.16105875394277...|       1.0|\n",
      "|[1.80554954312255...|            1|[5.75928989893551...|[0.11518579797871...|       1.0|\n",
      "|[2.21590171201404...|            1|[10.1115264360994...|[0.20223052872198...|       1.0|\n",
      "|[3.61109908624511...|            0|[22.4300693045137...|[0.44860138609027...|       1.0|\n",
      "|[2.29797214579234...|            1|[6.96604497781355...|[0.13932089955627...|       1.0|\n",
      "|[1.64140867556596...|            1|[5.35213631819035...|[0.10704272636380...|       1.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## run the whole pipeline\n",
    "pipe = Pipeline(stages=indexers+encoders+[va, ss, assembler, gbt])\n",
    "result = pipe.fit(df_aavail, paramMap).transform(df_aavail)\n",
    "result.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[is_subscriber: int, country: string, age: int, subscriber_type: string, num_streams: int, country_index: double, subscriber_type_index: double, country_oh: vector, subscriber_type_oh: vector, cont_features: vector, cont_scaled: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same procedure with a train-test split, cross-validations and grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train, test = df_aavail.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt = RandomForestClassifier(labelCol=\"is_subscriber\", featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 3]) \\\n",
    "    .addGrid(gbt.numTrees, [50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "pipe = Pipeline(stages=indexers+encoders+[va, ss, assembler])\n",
    "pipeline_model = pipe.fit(train)\n",
    "prepped_train = pipeline_model.transform(train)\n",
    "prepped_test = pipeline_model.transform(test)\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"is_subscriber\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(prepped_train)\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[-3.7470583658643...|            0|[43.1212677243314...|[0.43121267724331...|       1.0|\n",
      "|[1.22186685843403...|            0|[65.4764954611438...|[0.65476495461143...|       0.0|\n",
      "|[1.38478243955857...|            0|[42.5156472266135...|[0.42515647226613...|       1.0|\n",
      "|(5,[0,1],[1.38478...|            0|[63.9301262677363...|[0.63930126267736...|       0.0|\n",
      "|[1.46624023012084...|            0|[64.8543858101797...|[0.64854385810179...|       0.0|\n",
      "|[1.46624023012084...|            0|[62.7304327022885...|[0.62730432702288...|       0.0|\n",
      "|[1.54769802068311...|            0|[63.7622754717455...|[0.63762275471745...|       0.0|\n",
      "|(5,[0,1],[1.54769...|            0|[62.6302779420164...|[0.62630277942016...|       0.0|\n",
      "|[1.62915581124538...|            0|[63.0133830551275...|[0.63013383055127...|       0.0|\n",
      "|[1.62915581124538...|            0|[63.5774856192300...|[0.63577485619230...|       0.0|\n",
      "|(5,[0,1],[1.62915...|            0|[64.145714194915,...|[0.64145714194915...|       0.0|\n",
      "|(5,[0,1],[1.62915...|            0|[61.3607620202067...|[0.61360762020206...|       0.0|\n",
      "|(5,[0,1],[1.62915...|            0|[61.3099490165783...|[0.61309949016578...|       0.0|\n",
      "|[1.71061360180765...|            0|[47.4171893782578...|[0.47417189378257...|       1.0|\n",
      "|(5,[0,1],[1.71061...|            0|[67.5244425315943...|[0.67524442531594...|       0.0|\n",
      "|[1.79207139236992...|            0|[47.9174450904040...|[0.47917445090404...|       1.0|\n",
      "|(5,[0,1],[1.79207...|            0|[66.9053332808922...|[0.66905333280892...|       0.0|\n",
      "|(5,[0,1],[1.87352...|            0|[65.2775327224852...|[0.65277532722485...|       0.0|\n",
      "|(5,[0,1],[1.95498...|            0|[66.4938147043248...|[0.66493814704324...|       0.0|\n",
      "|(5,[0,1],[2.03644...|            0|[66.6681662893392...|[0.66668166289339...|       0.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(prepped_test)\n",
    "result = prediction.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\", \"prediction\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'is_subscriber'>, Column<b'rawPrediction'>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = prediction.select(\"is_subscriber\", \"rawPrediction\")\n",
    "list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5681] at RDD at PythonRDD.scala:53\n",
      "Area under PR = 0.862475204506754\n",
      "Area under ROC = 0.7738909517295289\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "predictionLabelsRDD = prediction.select('is_subscriber','prediction').rdd.map(lambda row: (float(row['prediction']), float(row['is_subscriber'])))\n",
    "# Instantiate metrics object\n",
    "print(predictionLabelsRDD)\n",
    "metrics = BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.evaluation.BinaryClassificationMetrics at 0x7f72b082eac0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
