{
 "cells": [
  {
   "attachments": {
    "ibm-cloud.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![ibm-cloud.png](attachment:ibm-cloud.png)\n",
    "\n",
    "# Spark  Supervised Machine Learning\n",
    "\n",
    "Keep the main [Spark ML documentation](https://spark.apache.org/docs/latest/ml-pipeline.html) as you go through this tutorial.  MLlib is Sparkâ€™s machine learning (ML) library. **Spark ML** is not an official name, but we will use it to refer to the MLlib DataFrame-based API that embraces ML pipelines. Before we get into Spark ML by demonstrating a couple of examples we will first review Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\",\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "What is Spark SQL?\n",
    "- Spark SQL takes basic RDDs and **puts a schema on them**.\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- Think of a DataFrames as **RDDs with schema**.\n",
    "\n",
    "What are **schemas**?\n",
    "- Schemas are metadata about your data.\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using **column names** instead of column positions\n",
    "- Schemas enable **queries** using SQL and DataFrame syntax\n",
    "- Schemas make your data more **structured**.\n",
    "\n",
    "See the [Spark SQL documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html) as a main point of reference for Spark SQL, DataFrames and Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), if you add a schema.\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspqrk.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module.  \n",
    "\n",
    "<center>\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "      <th>Types</th>\n",
    "      <th>Python Equivalent</th>\n",
    "    </tr>\n",
    "  <tr>\n",
    "      <td>StringType</td>\n",
    "      <td>string</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td>IntegerType</td>\n",
    "      <td>integer</td>\n",
    "   <tr>\n",
    "      <td>FloatType</td>\n",
    "      <td>float</td> \n",
    "  <tr>\n",
    "      <td>ArrayType</td>\n",
    "      <td>array or list</td>\n",
    "   </tr>\n",
    "    <tr>\n",
    "      <td>MapType</td>\n",
    "      <td>dict</td>\n",
    "   </tr>       \n",
    "</table>\n",
    "</center>\n",
    "\n",
    "First we initialize the Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"spark-ml-examples\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `local[4]` will create a `local` cluster made of the driver using all 4 cores.  Lets start with a very small file to to demonstrate the different ways to create Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(111, '10/13/2019', 4, 'US', 1),\n",
       " (122, '10/15/2019', 5, 'SG', 1),\n",
       " (102, '10/16/2019', 11, 'US', 1),\n",
       " (144, '10/25/2019', 14, 'US', 2),\n",
       " (121, '10/26/2019', 7, 'SG', 1),\n",
       " (155, '10/27/2019', 9, 'US', 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(args):\n",
    "    user_id, date, num_streams, country, invoice_item = args\n",
    "    return((int(user_id), date, int(num_streams), country, int(invoice_item)))\n",
    "\n",
    "rdd_aavail = sc.textFile(os.path.join(DATA_DIR, 'example-data.csv'))\\\n",
    "                         .map(lambda rowstr : rowstr.split(\",\"))\\\n",
    "                         .filter(lambda row: not row[0].startswith('#'))\\\n",
    "                         .map(casting_function)\n",
    "\n",
    "rdd_aavail.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You can create a Spark DataFrame using a schema that you have defined or it can be inferred.  To create your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-------+-------------+\n",
      "|user_id|      date|num_streams|country|invoice_items|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "|    111|10/13/2019|          4|     US|            1|\n",
      "|    122|10/15/2019|          5|     SG|            1|\n",
      "|    102|10/16/2019|         11|     US|            1|\n",
      "|    144|10/25/2019|         14|     US|            2|\n",
      "|    121|10/26/2019|          7|     SG|            1|\n",
      "|    155|10/27/2019|          9|     US|            3|\n",
      "+-------+----------+-----------+-------+-------------+\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_items: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_id', IntegerType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('num_streams', IntegerType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('invoice_items', IntegerType(), True) ])\n",
    "    \n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_aavail, schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also read the data directly from a file and **infer** the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line count: 6\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n",
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv(os.path.join(DATA_DIR, 'example-data.csv'),\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(df.count()))\n",
    "\n",
    "# show the table in a nice format\n",
    "df.show()\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn the DataFrame into a Panda DataFrame, but be careful since this 'action' will put all the data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#user</th>\n",
       "      <th>date</th>\n",
       "      <th>num_streams</th>\n",
       "      <th>country</th>\n",
       "      <th>invoice_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>10/13/2019</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>10/15/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>10/16/2019</td>\n",
       "      <td>11</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144</td>\n",
       "      <td>10/25/2019</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>7</td>\n",
       "      <td>SG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>10/27/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #user        date  num_streams country  invoice_item\n",
       "0    111  10/13/2019            4      US             1\n",
       "1    122  10/15/2019            5      SG             1\n",
       "2    102  10/16/2019           11      US             1\n",
       "3    144  10/25/2019           14      US             2\n",
       "4    121  10/26/2019            7      SG             1\n",
       "5    155  10/27/2019            9      US             3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common operations that you might perform on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #user: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_streams: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- invoice_item: integer (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|#user|      date|num_streams|country|invoice_item|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "|  111|10/13/2019|          4|     US|           1|\n",
      "|  122|10/15/2019|          5|     SG|           1|\n",
      "|  102|10/16/2019|         11|     US|           1|\n",
      "|  144|10/25/2019|         14|     US|           2|\n",
      "|  121|10/26/2019|          7|     SG|           1|\n",
      "|  155|10/27/2019|          9|     US|           3|\n",
      "+-----+----------+-----------+-------+------------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|summary|             #user|      date|      num_streams|country|      invoice_item|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "|  count|                 6|         6|                6|      6|                 6|\n",
      "|   mean|125.83333333333333|      null|8.333333333333334|   null|               1.5|\n",
      "| stddev|20.034137532388726|      null|3.777124126457412|   null|0.8366600265340756|\n",
      "|    min|               102|10/13/2019|                4|     SG|                 1|\n",
      "|    max|               155|10/27/2019|               14|     US|                 3|\n",
      "+-------+------------------+----------+-----------------+-------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+-----------------+\n",
      "|summary|      num_streams|\n",
      "+-------+-----------------+\n",
      "|  count|                6|\n",
      "|   mean|8.333333333333334|\n",
      "| stddev|3.777124126457412|\n",
      "|    min|                4|\n",
      "|    max|               14|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df.describe(\"num_streams\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "\n",
    "Lets read in in the AAVAIL dataset that we have been working with to demonstrate the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|summary|      customer_id|     is_subscriber|      country|               age| customer_name| subscriber_type|      num_streams|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "|  count|             1000|              1000|         1000|              1000|          1000|            1000|             1000|\n",
      "|   mean|            500.5|             0.711|         null|            25.325|          null|            null|           17.695|\n",
      "| stddev|288.8194360957494|0.4535247343692345|         null|12.184655959067568|          null|            null|4.798020007877829|\n",
      "|    min|                1|                 0|    singapore|               -50|Aaliyah Duarte|    aavail_basic|                1|\n",
      "|    max|             1000|                 1|united_states|                50|   Zoie Cortes|aavail_unlimited|               29|\n",
      "+-------+-----------------+------------------+-------------+------------------+--------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aavail = spark.read.csv(os.path.join(DATA_DIR, 'aavail-target.csv'),\n",
    "                           header=True,       \n",
    "                           quote='\"',         \n",
    "                           sep=\",\",          \n",
    "                           inferSchema=True)\n",
    "df_aavail.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Remove one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|summary|     is_subscriber|      country|               age| subscriber_type|      num_streams|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "|  count|              1000|         1000|              1000|            1000|             1000|\n",
      "|   mean|             0.711|         null|            25.325|            null|           17.695|\n",
      "| stddev|0.4535247343692345|         null|12.184655959067568|            null|4.798020007877829|\n",
      "|    min|                 0|    singapore|               -50|    aavail_basic|                1|\n",
      "|    max|                 1|united_states|                50|aavail_unlimited|               29|\n",
      "+-------+------------------+-------------+------------------+----------------+-----------------+\n",
      "\n",
      "+----------------+-----+\n",
      "| subscriber_type|count|\n",
      "+----------------+-----+\n",
      "|  aavail_premium|  331|\n",
      "|aavail_unlimited|  302|\n",
      "|    aavail_basic|  367|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['customer_id', 'customer_name']\n",
    "df_aavail = df_aavail.drop(*columns_to_drop)\n",
    "df_aavail.describe().show()\n",
    "df_aavail.groupBy(\"subscriber_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+---+----------------+-----------+\n",
      "|is_subscriber|      country|age| subscriber_type|num_streams|\n",
      "+-------------+-------------+---+----------------+-----------+\n",
      "|            1|united_states| 21|  aavail_premium|         23|\n",
      "|            0|    singapore| 30|aavail_unlimited|         12|\n",
      "|            0|united_states| 21|  aavail_premium|         22|\n",
      "|            1|united_states| 20|    aavail_basic|         19|\n",
      "|            1|    singapore| 21|  aavail_premium|         23|\n",
      "|            1|united_states| 21|  aavail_premium|         20|\n",
      "|            0|    singapore| 48|    aavail_basic|         18|\n",
      "|            1|united_states| 47|  aavail_premium|         20|\n",
      "|            0|united_states| 21|  aavail_premium|         24|\n",
      "|            0|united_states| 26|    aavail_basic|         20|\n",
      "|            1|united_states| 14|  aavail_premium|         18|\n",
      "|            0|    singapore| 39|aavail_unlimited|          1|\n",
      "|            1|united_states| 20|  aavail_premium|         21|\n",
      "|            1|united_states| 19|    aavail_basic|         22|\n",
      "|            0|united_states| 22|  aavail_premium|         20|\n",
      "|            1|united_states| 22|    aavail_basic|          5|\n",
      "|            1|united_states| 27|  aavail_premium|         24|\n",
      "|            0|    singapore| 44|    aavail_basic|         17|\n",
      "|            1|united_states| 28|  aavail_premium|         19|\n",
      "|            1|united_states| 20|  aavail_premium|          5|\n",
      "+-------------+-------------+---+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aavail.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on a feature matrix\n",
    "\n",
    "The following example demonstrates how to deal with categorical features and scale continuous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## scale the continuous features\n",
    "va = VectorAssembler(inputCols=[\"age\", \"num_streams\"], outputCol=\"cont_features\")\n",
    "ss = MinMaxScaler(inputCol=\"cont_features\", outputCol=\"cont_scaled\")\n",
    "\n",
    "\n",
    "\n",
    "## categorical variable transformation\n",
    "cat_cols = [\"country\", \"subscriber_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_oh\") for column in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "## assemple the features for input into the ML model\n",
    "assembler = VectorAssembler(inputCols=[\"cont_scaled\", \"country_oh\", \"subscriber_type_oh\"], outputCol=\"features\")\n",
    "\n",
    "## setup a model\n",
    "gbt = NaiveBayes(labelCol=\"is_subscriber\", featuresCol=\"features\")\n",
    "paramMap = {gbt.smoothing: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[is_subscriber: int, country: string, age: int, subscriber_type: string, num_streams: int]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aavail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the pipeline and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the whole pipeline\n",
    "pipe = Pipeline(stages=indexers+encoders+[va, ss, assembler])\n",
    "model = pipe.fit(df_aavail, paramMap)\n",
    "result = model.transform(df_aavail)\n",
    "#result.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.71,0.785714285...|\n",
      "|(5,[0,1],[0.8,0.3...|\n",
      "|[0.71,0.75,1.0,0....|\n",
      "|[0.70000000000000...|\n",
      "|[0.71,0.785714285...|\n",
      "|[0.71,0.678571428...|\n",
      "|[0.98,0.607142857...|\n",
      "|[0.97,0.678571428...|\n",
      "|[0.71,0.821428571...|\n",
      "|[0.76,0.678571428...|\n",
      "|[0.64,0.607142857...|\n",
      "|      (5,[0],[0.89])|\n",
      "|[0.70000000000000...|\n",
      "|[0.69000000000000...|\n",
      "|[0.72,0.678571428...|\n",
      "|[0.72,0.142857142...|\n",
      "|[0.77,0.821428571...|\n",
      "|[0.94000000000000...|\n",
      "|[0.78,0.642857142...|\n",
      "|[0.70000000000000...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[0.71,0.785714285...|            1|[-6.8882711018806...|[0.28730445923614...|       1.0|\n",
      "|(5,[0,1],[0.8,0.3...|            0|[-2.7016890688835...|[0.34042616474719...|       1.0|\n",
      "|[0.71,0.75,1.0,0....|            0|[-6.8378085066757...|[0.28617076242146...|       1.0|\n",
      "|[0.70000000000000...|            1|[-6.8393896703151...|[0.21511681147930...|       1.0|\n",
      "|[0.71,0.785714285...|            1|[-5.0696349158897...|[0.41398066341604...|       1.0|\n",
      "|[0.71,0.678571428...|            1|[-6.7368833162658...|[0.28391142968340...|       1.0|\n",
      "|[0.98,0.607142857...|            0|[-5.2876457461570...|[0.33684639194208...|       1.0|\n",
      "|[0.97,0.678571428...|            1|[-7.0315699692295...|[0.29567101039979...|       1.0|\n",
      "|[0.71,0.821428571...|            0|[-6.9387336970856...|[0.28844083250560...|       1.0|\n",
      "|[0.76,0.678571428...|            0|[-6.9578568777424...|[0.21829611355419...|       1.0|\n",
      "|[0.64,0.607142857...|            1|[-6.5566194115965...|[0.27856059821609...|       1.0|\n",
      "|      (5,[0],[0.89])|            0|[-2.2486074399628...|[0.33123805010583...|       1.0|\n",
      "|[0.70000000000000...|            1|[-6.7760118094337...|[0.28459206300018...|       1.0|\n",
      "|[0.69000000000000...|            1|[-6.9794433538929...|[0.21756353080720...|       1.0|\n",
      "|[0.72,0.678571428...|            0|[-6.7482174183029...|[0.28435847153982...|       1.0|\n",
      "|[0.72,0.142857142...|            1|[-6.1555815415202...|[0.20301380537370...|       1.0|\n",
      "|[0.77,0.821428571...|            1|[-7.0067383093079...|[0.29115488617396...|       1.0|\n",
      "|[0.94000000000000...|            0|[-5.1918467428038...|[0.33365185926366...|       1.0|\n",
      "|[0.78,0.642857142...|            1|[-6.7657594353203...|[0.28591651402491...|       1.0|\n",
      "|[0.70000000000000...|            1|[-5.9686102861548...|[0.26688437870806...|       1.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = gbt.fit(result)\n",
    "nb_pred = nb.transform(result)\n",
    "nb_pred.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.340266 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"is_subscriber\", metricName=\"areaUnderROC\",\n",
    "                                         rawPredictionCol='rawPrediction')\n",
    "accuracy = evaluator.evaluate(nb_pred)\n",
    "print(\"AUC = %g \" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same procedure with a train-test split, cross-validations and grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train, test = df_aavail.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "gbt = NaiveBayes(labelCol=\"is_subscriber\", featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.smoothing, [1.0, 0.8]) \\\n",
    "    .build()\n",
    "\n",
    "pipe = Pipeline(stages=indexers+encoders+[va, ss, assembler])\n",
    "pipeline_model = pipe.fit(train)\n",
    "prepped_train = pipeline_model.transform(train)\n",
    "prepped_test = pipeline_model.transform(test)\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"is_subscriber\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(prepped_train)\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|is_subscriber|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|[0.04,0.607142857...|            0|[-4.2080035225333...|[0.30112761374337...|       1.0|\n",
      "|[0.65,0.5,0.0,0.0...|            0|[-4.5867740794712...|[0.39625947011427...|       1.0|\n",
      "|[0.67,0.25,0.0,1....|            0|[-4.4231205125499...|[0.31747813368022...|       1.0|\n",
      "|(5,[0,1],[0.67,0....|            0|[-2.4414127273597...|[0.33476839555953...|       1.0|\n",
      "|[0.68,0.285714285...|            0|[-4.3188923307313...|[0.38990079563100...|       1.0|\n",
      "|[0.68,0.678571428...|            0|[-4.8727455277062...|[0.40434522598237...|       1.0|\n",
      "|[0.69000000000000...|            0|[-4.5317001582892...|[0.39563104565363...|       1.0|\n",
      "|(5,[0,1],[0.69000...|            0|[-2.5649266386710...|[0.33814884517245...|       1.0|\n",
      "|[0.70000000000000...|            0|[-4.9962594390175...|[0.40799735116187...|       1.0|\n",
      "|[0.70000000000000...|            0|[-5.0466097296516...|[0.40932288317581...|       1.0|\n",
      "|(5,[0],[0.7000000...|            0|[-2.0224801067176...|[0.32523734218654...|       1.0|\n",
      "|(5,[0,1],[0.70000...|            0|[-2.8784350474971...|[0.34602508856098...|       1.0|\n",
      "|(5,[0,1],[0.70000...|            0|[-2.9791356287653...|[0.34851176693306...|       1.0|\n",
      "|[0.71,0.607142857...|            0|[-4.9722500789770...|[0.33132665148544...|       1.0|\n",
      "|(5,[0,1],[0.71,0....|            0|[-2.3863388061778...|[0.33418351083799...|       1.0|\n",
      "|[0.72,0.678571428...|            0|[-5.0843573252667...|[0.33422556043357...|       1.0|\n",
      "|(5,[0,1],[0.72,0....|            0|[-2.4984460524675...|[0.33709481458421...|       1.0|\n",
      "|(5,[0,1],[0.73,0....|            0|[-2.9126550425617...|[0.34744239152348...|       1.0|\n",
      "|(5,[0,1],[0.74,0....|            0|[-2.5716096731447...|[0.33925608401145...|       1.0|\n",
      "|(5,[0,1],[0.75,0....|            0|[-2.5326660475321...|[0.33849447215765...|       1.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = cvModel.transform(prepped_test)\n",
    "result = prediction.select(\"features\", \"is_subscriber\", \"rawPrediction\", \"probability\", \"prediction\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'is_subscriber'>, Column<b'rawPrediction'>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = prediction.select(\"is_subscriber\", \"rawPrediction\")\n",
    "list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1398] at RDD at PythonRDD.scala:53\n",
      "Area under PR = 0.7345679012345679\n",
      "Area under ROC = 0.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "predictionLabelsRDD = prediction.select('is_subscriber','prediction').rdd.map(lambda row: (float(row['prediction']), float(row['is_subscriber'])))\n",
    "# Instantiate metrics object\n",
    "print(predictionLabelsRDD)\n",
    "metrics = BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3447587038374404, 0.3447587038374404]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
