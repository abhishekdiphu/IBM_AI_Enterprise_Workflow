{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Tutorial\n",
    "\n",
    "Before you get started.  If you feel like you still need some practice getting a feel for docker try the tutrial for beginners in the Docker tutorial, before starting this tutorial.  Docker is intuitive so going through a few examples will be all that it takes to get comfortable.\n",
    "\n",
    "* [Docker Tutorials](https://github.com/docker/labs/blob/master/beginner/readme.md)\n",
    "\n",
    "This tutorial is loosly based on the second tutorial, `Webapps with Docker`, so going through both of those tutorials along with this one will provide a lot of context for how to use Docker in a number of different ways.\n",
    "\n",
    "> You will need to run through this tutorial with access to a termminal.  Jupyter lab or an open terminal will work.  We will create some of the files you need from within this notebook, but Docker is a command line tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "\n",
    "if not os.path.exists(\"models\") :\n",
    "    os.mkdir(\"models\")\n",
    "MODEL_DIR = \"models\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing pipeline\n",
    "numeric_features = ['age', 'num_streams']\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['country', 'subscriber_type']\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                               ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "def load_aavail_data():\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, r\"aavail-target.csv\"))\n",
    "\n",
    "    ## pull out the target and remove uneeded columns\n",
    "    _y = df.pop('is_subscriber')\n",
    "    y = np.zeros(_y.size)\n",
    "    y[_y==0] = 1 \n",
    "    df.drop(columns=['customer_id','customer_name'],inplace=True)\n",
    "    return(df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker command reference\n",
    "\n",
    "Here is a quick reference to keep your Docker commands accessible.\n",
    "\n",
    "| command | description |\n",
    "|:--|:--|\n",
    "|`docker container ls`| # List all running containers|\n",
    "|`docker ps` | # List all running containers|\n",
    "|`docker container ls -a` |  # List all containers, even those not running|\n",
    "|`docker container stop CONTAINER_ID_OR_NAME` | # Gracefully stop the specified container|\n",
    "|`docker container kill CONTAINER_ID_OR_NAME` | # Force shutdown of the specified container|\n",
    "|`docker container rm CONTAINER_ID_OR_NAME`  |   # Remove specified container from this machine|\n",
    "|`docker container rm $(docker container ls -a -q)` | # Remove all containers|\n",
    "|`docker image ls -a`  | # List all images on this machine|\n",
    "|`docker image rm IMAGE_ID_OR_NAME` | # Remove specified image from this machine|\n",
    "|`docker image rm $(docker image ls -a -q)`   |# Remove all images from this machine|\n",
    "|`docker login` |# Log in this CLI session using your Docker credentials|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist a machine learning model\n",
    "\n",
    "Vist the docs to learn more about [model persistence in scikit-learn](https://scikit-learn.org/stable/modules/model_persistence.html).  Be careful with sensitive data and pickle files since the data can easily be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.87       142\n",
      "         1.0       0.71      0.55      0.62        58\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       200\n",
      "   macro avg       0.77      0.73      0.75       200\n",
      "weighted avg       0.80      0.81      0.80       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models\\\\aavail-rf.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load data (you may need to adjust the location of the data to match your system)\n",
    "X,y = load_aavail_data()\n",
    "\n",
    "## train test split check model performance (assumes you have already grid-searched to tune model)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "params = {'n_estimators': 100,'max_depth':2}   \n",
    "clf = ensemble.RandomForestClassifier(**params)\n",
    "pipe = Pipeline(steps=[('pre', preprocessor),\n",
    "                       ('clf',clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "## retrain using all of the data\n",
    "pipe.fit(X, y)\n",
    "saved_model = 'aavail-rf.joblib'\n",
    "joblib.dump(pipe, os.path.join(MODEL_DIR, saved_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "import joblib\n",
    "import socket\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    html = \"<h3>Hello {name}!</h3>\" \\\n",
    "           \"<b>Hostname:</b> {hostname}<br/>\"\n",
    "    return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname())\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    \n",
    "    ## input checking\n",
    "    if not request.json:\n",
    "        print(\"ERROR: API (predict): did not receive request data\")\n",
    "        return jsonify([])\n",
    "\n",
    "    query = request.json\n",
    "    query = pd.DataFrame(query)\n",
    "    \n",
    "    if len(query.shape) == 1:\n",
    "         query = query.reshape(1, -1)\n",
    "\n",
    "    y_pred = model.predict(query)\n",
    "    \n",
    "    return(jsonify(y_pred.tolist()))        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    saved_model = 'aavail-rf.joblib'\n",
    "    model = joblib.load(os.path.join(MODEL_DIR, saved_model))\n",
    "    app.run(host='0.0.0.0', port=8080,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the flask app\n",
    "\n",
    "Move into your `docker-tutorial` directory and start the app \n",
    "\n",
    "\n",
    "```bash\n",
    "$ python app.py\n",
    "```\n",
    "\n",
    "Then go to [http://0.0.0.0:8080/](http://0.0.0.0:8080/)\n",
    "\n",
    "Stop the server.  We will relaunch it in a few moments from within Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DockerFile\n",
    "\n",
    "Before we build the DockerFile first we need to create a requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "cython\n",
    "numpy\n",
    "flask\n",
    "pandas\n",
    "scikit-learn==0.20.4\n",
    "joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.7.5\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "python3-dev \\\n",
    "build-essential    \n",
    "        \n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "ADD . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -U -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Docker image and run it\n",
    "\n",
    "Step one: build the image (from the directory that was created with this notebook)\n",
    " \n",
    "```bash\n",
    "    ~$ cd docker-tutorial\n",
    "    ~$ docker build -t example-ml-app .\n",
    "```\n",
    "\n",
    "Check that the image is there.\n",
    "\n",
    "```bash\n",
    "    ~$ docker image ls\n",
    "```\n",
    "\n",
    "You may notice images that you no longer use.  You may delete them with\n",
    "\n",
    "```bash\n",
    "    ~$ docker image rm IMAGE_ID_OR_NAME\n",
    "```\n",
    "\n",
    "Run the container\n",
    "\n",
    "```bash\n",
    "docker run -p 4000:8080 example-ml-app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the running app\n",
    "\n",
    "First go to [http://0.0.0.0:4000/](http://0.0.0.0:4000/) to ensure the app is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>subscriber_type</th>\n",
       "      <th>num_streams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>united_states</td>\n",
       "      <td>28</td>\n",
       "      <td>aavail_premium</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>united_states</td>\n",
       "      <td>30</td>\n",
       "      <td>aavail_basic</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>singapore</td>\n",
       "      <td>33</td>\n",
       "      <td>aavail_basic</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united_states</td>\n",
       "      <td>24</td>\n",
       "      <td>aavail_basic</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>singapore</td>\n",
       "      <td>39</td>\n",
       "      <td>aavail_unlimited</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country  age   subscriber_type  num_streams\n",
       "0  united_states   28    aavail_premium            9\n",
       "1  united_states   30      aavail_basic           19\n",
       "2      singapore   33      aavail_basic           14\n",
       "3  united_states   24      aavail_basic           33\n",
       "4      singapore   39  aavail_unlimited           20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create some new data\n",
    "X_new_data = {}\n",
    "X_new_data['country'] = ['united_states','united_states','singapore','united_states','singapore']\n",
    "X_new_data['age'] = [28,30,33,24,39]\n",
    "X_new_data['subscriber_type'] = ['aavail_premium','aavail_basic','aavail_basic','aavail_basic','aavail_unlimited']\n",
    "X_new_data['num_streams'] = [9,19,14,33,20]\n",
    "X_new = pd.DataFrame(X_new_data)\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "## data needs to be in dict format for JSON\n",
    "query = X_new.to_dict()\n",
    "\n",
    "## test the Flask API\n",
    "# port = 8080\n",
    "# r = requests.post('http://0.0.0.0:{}/predict'.format(port),json=query)\n",
    "\n",
    "## test the Docker API\n",
    "port = 4000\n",
    "r = requests.post('http://0.0.0.0:{}/predict'.format(port),json=query)\n",
    "\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued learning\n",
    "\n",
    "In this tutorial we showed how to add a `predict` endpoint to the flask app.  Go back and edit the flask app to add a training endpoint that accepts new data as input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
