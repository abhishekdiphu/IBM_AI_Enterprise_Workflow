{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY - performance monitoring\n",
    "\n",
    "You will be building your own workflow template in this tutorial.  You already have a Dockerfile and a basic Flask application to build an API.  Lets combine what you have learned about logging to build a ``workflow-template`` that can be used to deploy models in a way that facilitates performance monitoring.\n",
    "\n",
    "There are three main parts to this case study.\n",
    "\n",
    "1. Write unit tests for a logger and a logging API endpoint\n",
    "2. Add logging to your Docker container\n",
    "3. Add an API endpoint for logging\n",
    "4. Make sure all tests pass\n",
    "5. Create model performance investigative tooling\n",
    "6. Swap out the iris data for the AAVAIL churn data\n",
    "\n",
    "You may want to eventually rename the directory because in this case-study you will swap out the iris data for `aavail-target.csv`.  It reality you will eventually want a library of workflow templates to work from and the naming convention you decide on can help with organization.  This notebook should reside in that source directory regardless of the name.  We suggest that you go through all of the tasks **first** using the iris data **then** copy the template to a new folder and make it work for the AAVAIL churn data.  Eventually you will want a suite of workflow templates that you will be able to select from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The ``workflow-template.zip`` is a workflow template.  Unpack the directory in a location where you would like the source code to exist.  Leaving out the ``static`` directory that contains css and JavaScript to render a landing page, the important pieces are shown in the following tree.\n",
    "\n",
    "```\n",
    "├── app.py\n",
    "├── Dockerfile\n",
    "├── model.py\n",
    "├── README.rst\n",
    "├── requirements.txt\n",
    "├── run-tests.py\n",
    "├── templates\n",
    "│   ├── base.html\n",
    "│   ├── dashboard.html\n",
    "│   ├── index.html\n",
    "│   └── running.html\n",
    "└── unittests\n",
    "    ├── ApiTests.py\n",
    "    ├── __init__.py\n",
    "    ├── ModelTests.py\n",
    "```\n",
    "\n",
    "If you plan on modifying the HTML website you will need to modify the files in ``templates``.  The rest of the files you should be familiar with at this point.\n",
    "\n",
    "\n",
    "We will be working with an Flask API to interact with our model. In order to access the different endpoints of this API make sure the app is running. Open a new command prompt and run the app with the command :\n",
    "\n",
    "```\n",
    "python path/to/working/directory/app.py -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Write units test for a logger\n",
    "\n",
    "1. Using `model.py` and `./unittests/ModelTests.py` as an example complete `logger.py` and \n",
    "`./unittests/LoggerTests.py`.\n",
    "2. Modify the files so that there are at a minimum the following tests:\n",
    "\n",
    "    * ensure predict log is automatically created\n",
    "    * ensure train log is automatically created\n",
    "    * ensure that content can be retrieved from predict log file\n",
    "    * ensure that content can be retrieved from train log file\n",
    "    \n",
    "> IMPORTANT: when writing to a log file from a unit test you will want to ensure that you do not modify or delete existing 'production' logs.  You can test your function with the following code (although it is likely easier to work directly in a terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Add an API endpoint for logging\n",
    "\n",
    "In addition to the `predict` and `train` endpoints, create a third endpoint that returns \n",
    "logs.  Remember that there are `train` and `predict` log files and that they are set up \n",
    "to create new files each month.  You will need to ensure that your endpoint can accommodate this and the best way to ensure this is to **first write the unit tests** then write the code.\n",
    "\n",
    "Flask has several functions to help with the sending of files. One example is [send_from_directory](https://flask.palletsprojects.com/en/1.1.x/api/#flask.send_from_directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API is ready we can test it. We invite you to take a close look into the ApiTests.py script.\n",
    "!python ./unittests/ApiTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Make sure all tests pass\n",
    "\n",
    "You have been working on specific suites of unit tests.  It is a best practice to double-check that all tests pass after making major changes like the ones you have just completed.\n",
    "\n",
    "> make sure you modify the `./unittests/__init__.py` so that the LoggerTest suite is also included when running all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run-tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Create model performance investigative tooling\n",
    "\n",
    "There are a lot of convenience functions you could create here.  Create them directly in this notebook or create them as scripts that you may call from this notebook.  \n",
    "\n",
    "First write a script that accomplishes the following:\n",
    "\n",
    "* train one model, then select another type of machine learning model and train again,  ensuring that each has separate version numbers.\n",
    "* simulate a couple of hundred predictions for each model.\n",
    "\n",
    "At minimum create a tablular summary and/or a simple plot that accomplishes the following:\n",
    "\n",
    "1. Compare model performance for the two models\n",
    "2. Determine if there was any drift from the first model to the second using a novelty detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint :*** The API has been built such that only dictionaries can be sent as query for the model. After training the model with the following command line :\n",
    "```\n",
    "python run-model-train.py\n",
    "```\n",
    "you will need to transtype your data into dictionaries in order to be able to call the predict endpoint of the API :\n",
    "```\n",
    "X_query_pd = pd.DataFrame([[5.1, 3.2], [10, 2.3]])\n",
    "request_json = {'query':X_query_pd.to_dict(), 'type':'dict'}\n",
    "port = 8080\n",
    "r = requests.post('http://127.0.0.1:{}/predict'.format(port), json=request_json)\n",
    "print(r.text)\n",
    "```\n",
    "You can of course tweak the API (directly in app.py) to accept more data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Swap out the iris data for the AAVAIL churn data\n",
    "\n",
    "We suggest that you copy the iris example folder to a another directory, then re-create the template to work with the AAVAIL data.  The exercise of changing the dataset is very much aligned with real-world practices since you will often be modifying workflow-templates to meet the needs of a particular business opportunity.\n",
    "\n",
    "Start by updating the model.py script to load the AAVAIL data, apply an appropriate preprocessing pipeline to this data and fit a classifier. Then, run the tests. Looking at the output of the test adapt the different scripts until all the tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run-tests.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
