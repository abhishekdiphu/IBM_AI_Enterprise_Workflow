{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY - performance monitoring\n",
    "\n",
    "You will be building your own workflow template in this tutorial.  You already have a Dockerfile and a basic Flask application to build an API.  Lets combine what you have learned about logging to build a ``workflow-template`` that can be used to deploy models in a way that facilitates performance monitoring.\n",
    "\n",
    "There are three main parts to this case study.\n",
    "\n",
    "1. Write unit tests for a logger and a logging API endpoint\n",
    "2. Add logging to your Docker container\n",
    "3. Add an API endpoint for logging\n",
    "4. Make sure all tests pass\n",
    "5. Create model performance investigative tooling\n",
    "6. Swap out the iris data for the AAVAIL churn data\n",
    "\n",
    "You may want to eventually rename the directory because in this case-study you will swap out the iris data for `aavail-target.csv`.  It reality you will eventually want a library of workflow templates to work from and the naming convention you decide on can help with organization.  This notebook should reside in that source directory regardless of the name.  We suggest that you go through all of the tasks **first** using the iris data **then** copy the template to a new folder and make it work for the AAVAIL churn data.  Eventually you will want a suite of workflow templates that you will be able to select from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "The ``iris-nologs.zip`` is a workflow template.  Unpack the directory in a location where you would like the source code to exist.  Leaving out the ``static`` directory that contains css and JavaScript to render a landing page, the important pieces are shown in the following tree.\n",
    "\n",
    "```\n",
    "├── app.py\n",
    "├── Dockerfile\n",
    "├── model.py\n",
    "├── README.rst\n",
    "├── requirements.txt\n",
    "├── run-tests.py\n",
    "├── templates\n",
    "│   ├── base.html\n",
    "│   ├── dashboard.html\n",
    "│   ├── index.html\n",
    "│   └── running.html\n",
    "└── unittests\n",
    "    ├── ApiTests.py\n",
    "    ├── __init__.py\n",
    "    ├── ModelTests.py\n",
    "```\n",
    "\n",
    "If you plan on modifying the HTML website you will need to modify the files in ``templates``.  The rest of the files you should be familiar with at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Write units test for a logger and a logging API endpoint\n",
    "\n",
    "1. Using `model.py` and `./unittests/ModelTests.py` as an example create `logger.py` and \n",
    "`./unittests/LoggerTests.py`.\n",
    "2. Modify the files so that there are at a minimum the following tests:\n",
    "\n",
    "    * ensure predict log is automatically created\n",
    "    * ensure train log is automaticall created\n",
    "    * ensure that train log archives last used training data\n",
    "    * ensure that 'n' predictions result in 'n' log entries\n",
    "    * ensure that predict gracefully handles NaNs\n",
    "    \n",
    "> IMPORTANT: when writing to a log file from a unit test you will want to ensure that you do not modify or delete existing 'production' logs.  You can test your function with the following code (although it is likely easier to work directly in a terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Add logging to your Docker container\n",
    "\n",
    "You may do this directly inside of `app.py` or your may separate out the logging code into the `logging.py` script.  Work on this task until all logging unit tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Add an API endpoint for logging\n",
    "\n",
    "In addition to the `predict` and `train` endpoints, create a third endpoint that returns \n",
    "logs.  Remember that there are `train` and `predict` log files and that they are set up \n",
    "to create new files each month.  You will need to ensure that your endpoint can accommodate this and the best way to ensure this is to **first write the unit tests** then write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./unittests/ApiTests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Make sure all tests pass\n",
    "\n",
    "You have been working on specific suites of unit tests.  It is a best practice to double-check that all tests pass after making major changes like the ones you have just completed.\n",
    "\n",
    "> make sure you modify the `./unittests/__init__.py` so that the LoggerTest suite is also included when running all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run-tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Create model performance investigative tooling\n",
    "\n",
    "There are a lot of convenience functions you could create here.  Create them directly in this notebook or create them as scripts that you may call from this notebook.  \n",
    "\n",
    "First write a script that accomplishes the following:\n",
    "\n",
    "* train one model, then select another type of machine learning model and train again,  ensuring that each has separate version numbers.\n",
    "* simulate a couple of hundred predictions for each model.\n",
    "\n",
    "At minimum create a tablular summary and/or a simple plot that accomplishes the following:\n",
    "\n",
    "1. Compare model performance for the two models\n",
    "2. Determine if there was any drift from the first model to the second using a novelty detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6: Swap out the iris data for the AAVAIL churn data\n",
    "\n",
    "We suggest that you copy the iris example folder to a another directory, then re-create the template to work with the AAVAIL data.  The exercise of changing the dataset is very much aligned with real-world practices since you will often be modifying workflow-templates to meet the needs of a particular business opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run-tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
